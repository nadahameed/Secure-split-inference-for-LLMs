# Secure-split-inference-for-LLMs
An implementation of secure split inference for large language models on resource-constrained edge devices.
