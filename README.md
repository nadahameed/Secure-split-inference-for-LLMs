# Secure Split Inference for Large Language Models
### An Implementation of Secure Split Inference for Large Language Models on Resource-Constrained Edge Devices

This project is an implementation of a secure, privacy-preserving computation offloading system for Large Language Model (LLM) inference. It demonstrates how a resource-constrained edge device (like a Raspberry Pi) can leverage a powerful server to run an LLM without exposing its private input data.

### Key Features
Privacy-Preserving Inference: Utilizes an Additive Secret Sharing protocol, functioning as a One-Time Pad, to ensure the server performs computations blindly on encrypted data.

Split Inference Architecture: The distilgpt2 model is split between a client (Raspberry Pi) and a server (Mac), offloading the most computationally intensive operations.

Hybrid Computation: Heavy linear layers are securely offloaded to the server, while non-linear operations (Layer Normalization, GELU, Softmax) are computed locally on the client.

End-to-End System: Provides a complete, working prototype from model preparation to final inference.

### File Descriptions
split_weights.py: A one-time utility script that deconstructs a pre-trained distilgpt2 model from the Hugging Face transformers library. It extracts the weight and bias for every linear layer and saves them into individual .npz files.

distilgpt2_weights/: This directory is generated by split_weights.py and contains all the individual weight files for the model. A complete copy is needed by both the client and the server.

server/llm_server.py: The server application that runs on a powerful machine (e.g., a Mac). It acts as a "blind calculator," loading the model weights and performing matrix multiplications on the encrypted data it receives from the client.

client/llm_client.py: The client application that runs on a resource-constrained device (e.g., a Raspberry Pi). It is the "orchestrator" of the entire process, handling data encryption, offloading requests, local non-linear computations, and final decryption.

### Setup and Installation
1. Create a virtual environment and install requirements.txt on your computer.

```
$python3 -m venv llm_env
$source llm_env/bin/activate
$pip install -r requirements.txt
```

2. Generate the model weights: Run the split_weights.py script, which will download the distilgpt2 model and create a new director named distilgpt2_weights containing all the necessary model files.

`$python split_weights.py`

3. Copy the newly created distilgpt2_weights folder into both the client and server directories.

```
$cp -r distilgpt2_weights client/
$cp -r distilgpt2_weights server/
```

4. Transfer the client folder as well as requirements.txt to your Raspberry Pi via tools like Cyberduck.

5. Set up a virtual environment on your Pi as well, and install requirements.txt.

### How to Run the Experiment
1. Start the Server: On your Mac/Laptop, navigate to the server/ directory and run the server script. Make sure the virtual environment is active.

`$python llm_server.py`

The server will start and wait for a connection.

2. Configure the Client: On your Raspberry Pi, open the client/llm_client.py file and edit the SERVER_HOST variable to be the IP address of your Mac.

3. Run the Client: On your Raspberry Pi, navigate to the client/ directory, activate its virtual environment, and run the client script.

`$python llm_client.py`

The client will connect to the server, and you will see the logs of the secure inference process in both terminals.
